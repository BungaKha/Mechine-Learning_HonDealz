{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow tensorflow-gpu\n",
        "!pip install tensorflow==2.15.0\n",
        "!pip install tensorflow-lite-support"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVISC-Ct5OAJ",
        "outputId": "ff8eb027-644b-45a0-b3cb-2a38ddbeb2dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.15.0\n",
            "Uninstalling tensorflow-2.15.0:\n",
            "  Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tensorflow==2.15.0\n",
            "  Using cached tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Using cached tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "Installing collected packages: tensorflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-2.15.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-lite-support (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-lite-support\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbfwkM-fq4il",
        "outputId": "c7e02589-1e8e-49ab-8203-9e7447105255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Cleaning data...\n",
            "Shape after dropping all-NaN rows: (1587, 7)\n",
            "Shape after cleaning prices: (1587, 7)\n",
            "Shape after cleaning mileage: (1587, 7)\n",
            "Shape after extracting engine size: (1587, 8)\n",
            "Final shape after cleaning: (1587, 9)\n",
            "\n",
            "Checking for remaining NaN values:\n",
            "model          0\n",
            "price          0\n",
            "year           0\n",
            "mileage        0\n",
            "location       0\n",
            "tax            0\n",
            "seller_type    0\n",
            "engine_size    0\n",
            "province       0\n",
            "dtype: int64\n",
            "Initial shape: (1587, 9)\n",
            "\n",
            "Training optimized model...\n",
            "Starting feature engineering...\n",
            "\n",
            "Feature Engineering Stats:\n",
            "Shape after feature engineering: (1587, 23)\n",
            "\n",
            "Training TensorFlow model...\n",
            "Epoch 1/100\n",
            "32/32 [==============================] - 5s 49ms/step - loss: 2137617839685632.0000 - mae: 22994432.0000 - val_loss: 768764182265856.0000 - val_mae: 21259442.0000\n",
            "Epoch 2/100\n",
            "32/32 [==============================] - 1s 46ms/step - loss: 2137616094855168.0000 - mae: 22994412.0000 - val_loss: 768762437435392.0000 - val_mae: 21259404.0000\n",
            "Epoch 3/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 2137612068323328.0000 - mae: 22994320.0000 - val_loss: 768754384371712.0000 - val_mae: 21259212.0000\n",
            "Epoch 4/100\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 2137592606752768.0000 - mae: 22993950.0000 - val_loss: 768725661777920.0000 - val_mae: 21258544.0000\n",
            "Epoch 5/100\n",
            "32/32 [==============================] - 1s 30ms/step - loss: 2137540127621120.0000 - mae: 22992892.0000 - val_loss: 768657881825280.0000 - val_mae: 21256952.0000\n",
            "Epoch 6/100\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 2137422016020480.0000 - mae: 22990578.0000 - val_loss: 768516147904512.0000 - val_mae: 21253618.0000\n",
            "Epoch 7/100\n",
            "32/32 [==============================] - 1s 26ms/step - loss: 2137207804526592.0000 - mae: 22986212.0000 - val_loss: 768265563406336.0000 - val_mae: 21247704.0000\n",
            "Epoch 8/100\n",
            "32/32 [==============================] - 1s 20ms/step - loss: 2136810788487168.0000 - mae: 22978608.0000 - val_loss: 767845193482240.0000 - val_mae: 21237774.0000\n",
            "Epoch 9/100\n",
            "32/32 [==============================] - 1s 19ms/step - loss: 2136195265986560.0000 - mae: 22966898.0000 - val_loss: 767240878161920.0000 - val_mae: 21223504.0000\n",
            "Epoch 10/100\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 2135309160546304.0000 - mae: 22949964.0000 - val_loss: 766353229217792.0000 - val_mae: 21202424.0000\n",
            "Epoch 11/100\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2134105495961600.0000 - mae: 22925964.0000 - val_loss: 765092454989824.0000 - val_mae: 21172600.0000\n",
            "Epoch 12/100\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2132383482511360.0000 - mae: 22892528.0000 - val_loss: 763446542991360.0000 - val_mae: 21133422.0000\n",
            "Epoch 13/100\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2130082051129344.0000 - mae: 22847736.0000 - val_loss: 761239466672128.0000 - val_mae: 21080760.0000\n",
            "Epoch 14/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2127048193605632.0000 - mae: 22790692.0000 - val_loss: 758465119125504.0000 - val_mae: 21014172.0000\n",
            "Epoch 15/100\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 2123083133485056.0000 - mae: 22714502.0000 - val_loss: 755021897531392.0000 - val_mae: 20931440.0000\n",
            "Epoch 16/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2118219217240064.0000 - mae: 22628234.0000 - val_loss: 750957247856640.0000 - val_mae: 20833048.0000\n",
            "Epoch 17/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2113222022791168.0000 - mae: 22524008.0000 - val_loss: 746058233675776.0000 - val_mae: 20713538.0000\n",
            "Epoch 18/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 2106337727086592.0000 - mae: 22391860.0000 - val_loss: 740086551412736.0000 - val_mae: 20567114.0000\n",
            "Epoch 19/100\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2099036685336576.0000 - mae: 22239222.0000 - val_loss: 733326004453376.0000 - val_mae: 20399880.0000\n",
            "Epoch 20/100\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 2089620774846464.0000 - mae: 22065734.0000 - val_loss: 725918796480512.0000 - val_mae: 20213472.0000\n",
            "Epoch 21/100\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 2081431077519360.0000 - mae: 21869572.0000 - val_loss: 717154781495296.0000 - val_mae: 19991908.0000\n",
            "Epoch 22/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2069898285023232.0000 - mae: 21646560.0000 - val_loss: 707540295876608.0000 - val_mae: 19744486.0000\n",
            "Epoch 23/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2056577980825600.0000 - mae: 21384452.0000 - val_loss: 697010780897280.0000 - val_mae: 19469128.0000\n",
            "Epoch 24/100\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2041319438417920.0000 - mae: 21094400.0000 - val_loss: 684801061289984.0000 - val_mae: 19145376.0000\n",
            "Epoch 25/100\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 2026674807898112.0000 - mae: 20782648.0000 - val_loss: 673051574272000.0000 - val_mae: 18823520.0000\n",
            "Epoch 26/100\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2008313017401344.0000 - mae: 20418912.0000 - val_loss: 658594177482752.0000 - val_mae: 18424252.0000\n",
            "Epoch 27/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 1989067805818880.0000 - mae: 20052788.0000 - val_loss: 645089592344576.0000 - val_mae: 18043992.0000\n",
            "Epoch 28/100\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1971417503498240.0000 - mae: 19664060.0000 - val_loss: 629989292638208.0000 - val_mae: 17620038.0000\n",
            "Epoch 29/100\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1952705136295936.0000 - mae: 19205622.0000 - val_loss: 614777256673280.0000 - val_mae: 17176416.0000\n",
            "Epoch 30/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1934194498338816.0000 - mae: 18748952.0000 - val_loss: 598871986143232.0000 - val_mae: 16699627.0000\n",
            "Epoch 31/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 1914079019008000.0000 - mae: 18275188.0000 - val_loss: 583333868208128.0000 - val_mae: 16214130.0000\n",
            "Epoch 32/100\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1892157908582400.0000 - mae: 17785604.0000 - val_loss: 567704549326848.0000 - val_mae: 15704043.0000\n",
            "Epoch 33/100\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1865857542127616.0000 - mae: 17279634.0000 - val_loss: 552077679919104.0000 - val_mae: 15179820.0000\n",
            "Epoch 34/100\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1840278998614016.0000 - mae: 16679627.0000 - val_loss: 535562926686208.0000 - val_mae: 14608733.0000\n",
            "Epoch 35/100\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1822236646309888.0000 - mae: 16153585.0000 - val_loss: 520119532912640.0000 - val_mae: 14051818.0000\n",
            "Epoch 36/100\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1801529669451776.0000 - mae: 15607872.0000 - val_loss: 506521834225664.0000 - val_mae: 13534227.0000\n",
            "Epoch 37/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1779153560928256.0000 - mae: 14960321.0000 - val_loss: 490721689731072.0000 - val_mae: 12907376.0000\n",
            "Epoch 38/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1753610853548032.0000 - mae: 14339809.0000 - val_loss: 477439134269440.0000 - val_mae: 12347994.0000\n",
            "Epoch 39/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1729885185769472.0000 - mae: 13784530.0000 - val_loss: 464681470787584.0000 - val_mae: 11777609.0000\n",
            "Epoch 40/100\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1720046892089344.0000 - mae: 13260189.0000 - val_loss: 453254475415552.0000 - val_mae: 11243057.0000\n",
            "Epoch 41/100\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 1685261549305856.0000 - mae: 12761588.0000 - val_loss: 443491612098560.0000 - val_mae: 10773112.0000\n",
            "Epoch 42/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1682100990246912.0000 - mae: 12140763.0000 - val_loss: 431760647127040.0000 - val_mae: 10184874.0000\n",
            "Epoch 43/100\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 1663931131101184.0000 - mae: 11630887.0000 - val_loss: 423911527284736.0000 - val_mae: 9760759.0000\n",
            "Epoch 44/100\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 1639439348531200.0000 - mae: 11210208.0000 - val_loss: 415225929007104.0000 - val_mae: 9280610.0000\n",
            "Epoch 45/100\n",
            "32/32 [==============================] - 1s 16ms/step - loss: 1617527499128832.0000 - mae: 10660035.0000 - val_loss: 406769742381056.0000 - val_mae: 8814073.0000\n",
            "Epoch 46/100\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 1617827341533184.0000 - mae: 10317763.0000 - val_loss: 399952018669568.0000 - val_mae: 8422207.0000\n",
            "Epoch 47/100\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 1607899390410752.0000 - mae: 9935503.0000 - val_loss: 394301116776448.0000 - val_mae: 8091008.5000\n",
            "Epoch 48/100\n",
            "32/32 [==============================] - 0s 16ms/step - loss: 1601242459537408.0000 - mae: 9748624.0000 - val_loss: 389429046804480.0000 - val_mae: 7794915.5000\n",
            "Epoch 49/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1566893190152192.0000 - mae: 9298961.0000 - val_loss: 384723205488640.0000 - val_mae: 7506777.0000\n",
            "Epoch 50/100\n",
            "32/32 [==============================] - 1s 16ms/step - loss: 1571292310405120.0000 - mae: 9184098.0000 - val_loss: 380822033006592.0000 - val_mae: 7286303.5000\n",
            "Epoch 51/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1560356518363136.0000 - mae: 8966348.0000 - val_loss: 377241137577984.0000 - val_mae: 7100523.0000\n",
            "Epoch 52/100\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1539746580922368.0000 - mae: 8812660.0000 - val_loss: 374342739296256.0000 - val_mae: 6970701.5000\n",
            "Epoch 53/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1519666937724928.0000 - mae: 8600240.0000 - val_loss: 371406726496256.0000 - val_mae: 6844339.5000\n",
            "Epoch 54/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1525044236779520.0000 - mae: 8507626.0000 - val_loss: 369602672459776.0000 - val_mae: 6760783.0000\n",
            "Epoch 55/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1522442828775424.0000 - mae: 8405741.0000 - val_loss: 367192826707968.0000 - val_mae: 6658641.5000\n",
            "Epoch 56/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1527725101678592.0000 - mae: 8309011.5000 - val_loss: 365246803869696.0000 - val_mae: 6584695.5000\n",
            "Epoch 57/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1504086675423232.0000 - mae: 8208666.0000 - val_loss: 363651189964800.0000 - val_mae: 6533577.5000\n",
            "Epoch 58/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1473441815330816.0000 - mae: 8186825.5000 - val_loss: 362287537848320.0000 - val_mae: 6492226.0000\n",
            "Epoch 59/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1462248056815616.0000 - mae: 8214816.0000 - val_loss: 361002067558400.0000 - val_mae: 6457945.5000\n",
            "Epoch 60/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1474835800653824.0000 - mae: 8234496.5000 - val_loss: 359750722125824.0000 - val_mae: 6431586.0000\n",
            "Epoch 61/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1506261942140928.0000 - mae: 8279247.0000 - val_loss: 358618025164800.0000 - val_mae: 6409973.5000\n",
            "Epoch 62/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1472914608095232.0000 - mae: 8165829.0000 - val_loss: 357317589598208.0000 - val_mae: 6375720.5000\n",
            "Epoch 63/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1438037057732608.0000 - mae: 8102324.0000 - val_loss: 356752230973440.0000 - val_mae: 6431279.5000\n",
            "Epoch 64/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1470839467802624.0000 - mae: 8225450.5000 - val_loss: 355802808320000.0000 - val_mae: 6432049.5000\n",
            "Epoch 65/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1430939490058240.0000 - mae: 8135903.0000 - val_loss: 354631892860928.0000 - val_mae: 6397872.5000\n",
            "Epoch 66/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1464115428065280.0000 - mae: 8136588.0000 - val_loss: 354041267748864.0000 - val_mae: 6429582.0000\n",
            "Epoch 67/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1457213147185152.0000 - mae: 8172294.0000 - val_loss: 353256731574272.0000 - val_mae: 6424290.5000\n",
            "Epoch 68/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1429874338168832.0000 - mae: 8144244.0000 - val_loss: 352419514941440.0000 - val_mae: 6427174.5000\n",
            "Epoch 69/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1404148289372160.0000 - mae: 8126824.5000 - val_loss: 351397178507264.0000 - val_mae: 6417966.5000\n",
            "Epoch 70/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1432432393846784.0000 - mae: 8215900.0000 - val_loss: 351027006013440.0000 - val_mae: 6459183.5000\n",
            "Epoch 71/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1407947053727744.0000 - mae: 8198993.5000 - val_loss: 349977121390592.0000 - val_mae: 6447439.0000\n",
            "Epoch 72/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1404036620222464.0000 - mae: 8096288.5000 - val_loss: 349251674570752.0000 - val_mae: 6455646.0000\n",
            "Epoch 73/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1404996679630848.0000 - mae: 8067410.0000 - val_loss: 348237122764800.0000 - val_mae: 6446317.0000\n",
            "Epoch 74/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1384441771458560.0000 - mae: 8131103.5000 - val_loss: 347471578398720.0000 - val_mae: 6455554.0000\n",
            "Epoch 75/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1375846870810624.0000 - mae: 8134481.0000 - val_loss: 346795792138240.0000 - val_mae: 6469700.0000\n",
            "Epoch 76/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1402780208070656.0000 - mae: 8237640.5000 - val_loss: 346330962591744.0000 - val_mae: 6492046.0000\n",
            "Epoch 77/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1364655528214528.0000 - mae: 8040014.0000 - val_loss: 345057471234048.0000 - val_mae: 6467164.0000\n",
            "Epoch 78/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1370983491436544.0000 - mae: 8125731.0000 - val_loss: 343919372009472.0000 - val_mae: 6451993.5000\n",
            "Epoch 79/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1371765712355328.0000 - mae: 8138361.5000 - val_loss: 343340289622016.0000 - val_mae: 6478019.0000\n",
            "Epoch 80/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1367196269805568.0000 - mae: 8071262.5000 - val_loss: 342141691756544.0000 - val_mae: 6457814.5000\n",
            "Epoch 81/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1366313922461696.0000 - mae: 8074656.0000 - val_loss: 341255754088448.0000 - val_mae: 6465044.5000\n",
            "Epoch 82/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1345329249124352.0000 - mae: 8029788.0000 - val_loss: 341179551973376.0000 - val_mae: 6526103.5000\n",
            "Epoch 83/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1349829837979648.0000 - mae: 8242122.0000 - val_loss: 340243685310464.0000 - val_mae: 6538237.0000\n",
            "Epoch 84/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1379266604302336.0000 - mae: 8107315.0000 - val_loss: 338638038630400.0000 - val_mae: 6484235.0000\n",
            "Epoch 85/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1317166209040384.0000 - mae: 7917904.0000 - val_loss: 338086974193664.0000 - val_mae: 6519471.5000\n",
            "Epoch 86/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1314117721784320.0000 - mae: 8127247.0000 - val_loss: 336757480488960.0000 - val_mae: 6505438.5000\n",
            "Epoch 87/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1324693676097536.0000 - mae: 8001290.0000 - val_loss: 336002975531008.0000 - val_mae: 6521611.0000\n",
            "Epoch 88/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1333678076592128.0000 - mae: 8087147.0000 - val_loss: 335236189650944.0000 - val_mae: 6540033.5000\n",
            "Epoch 89/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1303989517811712.0000 - mae: 8084794.5000 - val_loss: 334788103766016.0000 - val_mae: 6581474.0000\n",
            "Epoch 90/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1336544296173568.0000 - mae: 8060239.0000 - val_loss: 333631214059520.0000 - val_mae: 6575928.5000\n",
            "Epoch 91/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1310352646078464.0000 - mae: 7945183.0000 - val_loss: 332727358324736.0000 - val_mae: 6577359.5000\n",
            "Epoch 92/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1284483789619200.0000 - mae: 8063576.5000 - val_loss: 331461987139584.0000 - val_mae: 6579091.0000\n",
            "Epoch 93/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1284901877841920.0000 - mae: 8051533.0000 - val_loss: 330618697154560.0000 - val_mae: 6602425.5000\n",
            "Epoch 94/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1295061488762880.0000 - mae: 8056651.5000 - val_loss: 329778796167168.0000 - val_mae: 6623675.0000\n",
            "Epoch 95/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1276810159456256.0000 - mae: 8015259.0000 - val_loss: 327667417088000.0000 - val_mae: 6551547.0000\n",
            "Epoch 96/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1276762109509632.0000 - mae: 8193351.0000 - val_loss: 327919343763456.0000 - val_mae: 6649775.0000\n",
            "Epoch 97/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1254869050589184.0000 - mae: 8031458.5000 - val_loss: 327070383079424.0000 - val_mae: 6684869.5000\n",
            "Epoch 98/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1258183960035328.0000 - mae: 8025838.5000 - val_loss: 326159715794944.0000 - val_mae: 6700293.0000\n",
            "Epoch 99/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1305661199613952.0000 - mae: 8080143.0000 - val_loss: 323662460747776.0000 - val_mae: 6634607.5000\n",
            "Epoch 100/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1236923335049216.0000 - mae: 7913796.5000 - val_loss: 323094719758336.0000 - val_mae: 6682076.5000\n",
            "\n",
            "Training rf...\n",
            "rf Test R² Score: 0.9946\n",
            "rf CV R² Score: 0.7963\n",
            "\n",
            "Training xgb...\n",
            "xgb Test R² Score: 0.8408\n",
            "xgb CV R² Score: 0.7894\n",
            "\n",
            "Training gbm...\n",
            "gbm Test R² Score: 0.9975\n",
            "gbm CV R² Score: 0.7893\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "\n",
            "TensorFlow Test R² Score: 0.1765\n",
            "\n",
            "Final Ensemble R² Score: 0.8900\n",
            "\n",
            "Final Model Accuracy: 89.00%\n",
            "\n",
            "Saving model...\n",
            "Model saved successfully!\n",
            "\n",
            "Converting to TFLite...\n",
            "TFLite model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import tensorflow as tf\n",
        "from typing import Dict, Union\n",
        "\n",
        "def create_tf_model(input_dim):\n",
        "    \"\"\"Create a TensorFlow model for price prediction\"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_dim=input_dim),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_and_clean_data(df):\n",
        "    \"\"\"Clean and preprocess data with NaN handling\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Drop rows with all NaN values\n",
        "    df = df.dropna(how='all')\n",
        "    print(f\"Shape after dropping all-NaN rows: {df.shape}\")\n",
        "\n",
        "    # Clean price and drop rows with NaN prices\n",
        "    df['price'] = df['price'].astype(str).apply(lambda x: x.replace('Rp', '').replace('.', '').replace(',', '').strip())\n",
        "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
        "    df = df.dropna(subset=['price'])\n",
        "    print(f\"Shape after cleaning prices: {df.shape}\")\n",
        "\n",
        "    # Clean mileage\n",
        "    def clean_mileage(mileage):\n",
        "        try:\n",
        "            mileage = str(mileage)\n",
        "            if '-' in mileage:\n",
        "                start, end = map(lambda x: float(x.replace('km', '').replace(',', '').replace('.', '').strip()),\n",
        "                               mileage.split('-'))\n",
        "                return (start + end) / 2\n",
        "            return float(mileage.replace('km', '').replace(',', '').replace('.', '').strip())\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "    df['mileage'] = df['mileage'].apply(clean_mileage)\n",
        "    df['mileage'] = df['mileage'].fillna(df['mileage'].median())\n",
        "    print(f\"Shape after cleaning mileage: {df.shape}\")\n",
        "\n",
        "    # Extract engine size\n",
        "    def extract_engine_size(model):\n",
        "        model = str(model)\n",
        "        if '160' in model:\n",
        "            return 160\n",
        "        elif '150' in model:\n",
        "            return 150\n",
        "        elif '125' in model:\n",
        "            return 125\n",
        "        else:\n",
        "            return 110\n",
        "\n",
        "    df['engine_size'] = df['model'].apply(extract_engine_size)\n",
        "    print(f\"Shape after extracting engine size: {df.shape}\")\n",
        "\n",
        "    # Map locations to provinces\n",
        "    province_mapping = {\n",
        "        'Jakarta': ['Jakarta', 'Jakarta Timur', 'Jakarta Barat', 'Jakarta Selatan', 'Jakarta Utara', 'Jakarta Pusat'],\n",
        "        'Jawa Barat': ['Bandung', 'Depok', 'Bekasi', 'Bogor', 'Cimahi', 'Cianjur', 'Ciamis', 'Garut', 'Sukabumi', 'Karawang'],\n",
        "        'Banten': ['Tangerang', 'Tangerang Selatan', 'Serang', 'Cilegon'],\n",
        "        'Jawa Tengah': ['Semarang', 'Magelang', 'Klaten', 'Pemalang'],\n",
        "        'Yogyakarta': ['Yogyakarta', 'Sleman', 'Bantul'],\n",
        "        'Jawa Timur': ['Surabaya', 'Malang', 'Sidoarjo', 'Gresik', 'Kediri'],\n",
        "        'Bali': ['Denpasar', 'Badung', 'Buleleng']\n",
        "    }\n",
        "\n",
        "    def map_location_to_province(location):\n",
        "        location = str(location).lower()\n",
        "        for province, cities in province_mapping.items():\n",
        "            if any(city.lower() in location for city in cities):\n",
        "                return province\n",
        "        return 'Others'\n",
        "\n",
        "    df['province'] = df['location'].apply(map_location_to_province)\n",
        "    print(f\"Final shape after cleaning: {df.shape}\")\n",
        "\n",
        "    print(\"\\nChecking for remaining NaN values:\")\n",
        "    print(df.isna().sum())\n",
        "\n",
        "    return df\n",
        "\n",
        "def engineer_advanced_features(df):\n",
        "    \"\"\"Create sophisticated features with NaN handling\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Current year for age calculation\n",
        "    current_year = 2024\n",
        "    df['age'] = current_year - df['year']\n",
        "\n",
        "    # Advanced numerical features\n",
        "    df['age_squared'] = df['age'] ** 2\n",
        "    df['mileage_squared'] = df['mileage'] ** 2\n",
        "    df['price_per_cc'] = df['price'] / df['engine_size']\n",
        "    df['mileage_per_age'] = df['mileage'] / (df['age'] + 1)\n",
        "    df['engine_age_interaction'] = df['engine_size'] * np.exp(-df['age']/3)\n",
        "    df['normalized_mileage'] = df['mileage'] / (df['age'] + 1)\n",
        "    df['depreciation_factor'] = np.exp(-df['age']/5)\n",
        "\n",
        "    # Categorical features\n",
        "    df['price_segment'] = pd.qcut(df['price'], q=5,\n",
        "                                labels=['very_low', 'low', 'medium', 'high', 'very_high'],\n",
        "                                duplicates='drop')\n",
        "\n",
        "    df['age_category'] = pd.qcut(df['age'], q=4,\n",
        "                               labels=['new', 'medium_new', 'medium_old', 'old'],\n",
        "                               duplicates='drop')\n",
        "\n",
        "    # Market segment features\n",
        "    df['is_abs'] = df['model'].str.contains('ABS', case=False, na=False).astype(int)\n",
        "    df['is_cbs'] = df['model'].str.contains('CBS|ISS', case=False, na=False).astype(int)\n",
        "    df['is_premium'] = ((df['engine_size'] >= 150) |\n",
        "                       (df['model'].str.contains('ABS|CBS', case=False, na=False))).astype(int)\n",
        "\n",
        "    # Location based features\n",
        "    location_mean_price = df.groupby('province')['price'].transform('mean')\n",
        "    df['location_price_ratio'] = df['price'] / location_mean_price\n",
        "\n",
        "    print(\"\\nFeature Engineering Stats:\")\n",
        "    print(f\"Shape after feature engineering: {df.shape}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def train_optimized_model(df):\n",
        "    \"\"\"Train an optimized model ensemble including TensorFlow\"\"\"\n",
        "    print(\"Starting feature engineering...\")\n",
        "    try:\n",
        "        # Clean and engineer features\n",
        "        df = engineer_advanced_features(df)\n",
        "\n",
        "        # Prepare features\n",
        "        categorical_columns = ['price_segment', 'age_category', 'province']\n",
        "        numerical_columns = ['engine_size', 'year', 'mileage', 'age', 'mileage_per_age',\n",
        "                           'engine_age_interaction', 'location_price_ratio',\n",
        "                           'is_abs', 'is_cbs', 'is_premium', 'age_squared', 'mileage_squared',\n",
        "                           'price_per_cc', 'normalized_mileage', 'depreciation_factor']\n",
        "\n",
        "        # Encode categorical variables\n",
        "        encoded_df = pd.get_dummies(df[categorical_columns])\n",
        "\n",
        "        # Combine features\n",
        "        X = pd.concat([df[numerical_columns], encoded_df], axis=1)\n",
        "        y = df['price']\n",
        "\n",
        "        # Scale features for TensorFlow\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "        X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(\n",
        "            X_scaled_df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Create and train traditional models\n",
        "        models = {\n",
        "            'rf': RandomForestRegressor(n_estimators=300, max_depth=20, n_jobs=-1, random_state=42),\n",
        "            'xgb': XGBRegressor(n_estimators=300, max_depth=8, learning_rate=0.01, random_state=42),\n",
        "            'gbm': GradientBoostingRegressor(n_estimators=300, max_depth=7, learning_rate=0.01, random_state=42)\n",
        "        }\n",
        "\n",
        "        # Create and train TensorFlow model\n",
        "        tf_model = create_tf_model(X_train_scaled.shape[1])\n",
        "\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        print(\"\\nTraining TensorFlow model...\")\n",
        "        tf_model.fit(\n",
        "            X_train_scaled,\n",
        "            y_train,\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        predictions = {}\n",
        "        scores = {}\n",
        "\n",
        "        # Train and evaluate traditional models\n",
        "        for name, model in models.items():\n",
        "            print(f\"\\nTraining {name}...\")\n",
        "            model.fit(X_train, y_train)\n",
        "            pred = model.predict(X_test)\n",
        "            test_score = r2_score(y_test, pred)\n",
        "            cv_score = cross_val_score(model, X, y, cv=5, scoring='r2').mean()\n",
        "\n",
        "            predictions[name] = pred\n",
        "            scores[name] = (test_score + cv_score) / 2\n",
        "            print(f\"{name} Test R² Score: {test_score:.4f}\")\n",
        "            print(f\"{name} CV R² Score: {cv_score:.4f}\")\n",
        "\n",
        "        # Evaluate TensorFlow model\n",
        "        tf_pred = tf_model.predict(X_test_scaled).flatten()\n",
        "        tf_test_score = r2_score(y_test, tf_pred)\n",
        "        print(f\"\\nTensorFlow Test R² Score: {tf_test_score:.4f}\")\n",
        "\n",
        "        predictions['tf'] = tf_pred\n",
        "        scores['tf'] = tf_test_score\n",
        "\n",
        "        # Updated ensemble weights including TensorFlow\n",
        "        weights = {\n",
        "            'rf': 0.25,\n",
        "            'xgb': 0.35,\n",
        "            'gbm': 0.15,\n",
        "            'tf': 0.25\n",
        "        }\n",
        "\n",
        "        final_pred = sum(weights[name] * predictions[name] for name in weights.keys())\n",
        "        final_score = r2_score(y_test, final_pred)\n",
        "\n",
        "        print(f\"\\nFinal Ensemble R² Score: {final_score:.4f}\")\n",
        "\n",
        "        model_artifacts = {\n",
        "            'models': models,\n",
        "            'tf_model': tf_model,\n",
        "            'scaler': scaler,\n",
        "            'weights': weights,\n",
        "            'feature_columns': list(X.columns),\n",
        "            'categorical_columns': categorical_columns,\n",
        "            'numerical_columns': numerical_columns\n",
        "        }\n",
        "\n",
        "        return model_artifacts, final_score\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in model training: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Loading data...\")\n",
        "    df = pd.read_csv('datasetcapstone.csv')\n",
        "\n",
        "    print(\"Cleaning data...\")\n",
        "    df = load_and_clean_data(df)\n",
        "    print(f\"Initial shape: {df.shape}\")\n",
        "\n",
        "    print(\"\\nTraining optimized model...\")\n",
        "    model_artifacts, score = train_optimized_model(df)\n",
        "\n",
        "    print(f\"\\nFinal Model Accuracy: {score*100:.2f}%\")\n",
        "\n",
        "    print(\"\\nSaving model...\")\n",
        "    joblib.dump(model_artifacts, 'optimized_price_model_with_tf.joblib')\n",
        "    print(\"Model saved successfully!\")\n",
        "\n",
        "    # Convert to TFLite\n",
        "    print(\"\\nConverting to TFLite...\")\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model_artifacts['tf_model'])\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    # Save TFLite model\n",
        "    with open('vario_price_predictor.tflite', 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "    print(\"TFLite model saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YRwbt9XA6dcG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}